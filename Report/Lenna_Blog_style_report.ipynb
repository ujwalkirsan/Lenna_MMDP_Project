{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d717e13f",
   "metadata": {},
   "source": [
    "# Lenna: Language Enhanced Reasoning Detection Assistant\n",
    "**Author: Ujwal Kirsan**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654f742b",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "The motivation behind this project is to explore and enhance multimodal reasoning using vision-language models. With the rise of multimodal large language models (MLLMs), there is a growing demand for systems that can perform complex reasoning over images beyond simple captioning or classification. The Lenna framework attempts to tackle this by integrating a reasoning-based detection system that understands implicit language cues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472b3772",
   "metadata": {},
   "source": [
    "## Connection with Past and Current Multimodal Learning Work\n",
    "Multimodal learning has evolved from early image captioning systems to models capable of tasks like VQA (Visual Question Answering), REC (Referring Expression Comprehension), and more. Recent advances include DetGPT, BLIP-2, and MiniGPT-v2, which incorporate transformers for joint vision-language understanding. Lenna builds upon these by introducing a `<DET>` token that signals detection intent, improving implicit reasoning and localization through enhanced token embeddings and efficient training via LoRA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083605b3",
   "metadata": {},
   "source": [
    "## Learning from the Work\n",
    "From implementing Lenna and analyzing its architecture, I learned:\n",
    "- How a simple token injection (like `<DET>`) can guide multimodal models in specialized tasks.\n",
    "- The value of using aligned embeddings for semantic and positional features.\n",
    "- The significance of training data design (REC, VQA, ReasonDet) in guiding the model's reasoning capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cbc87a",
   "metadata": {},
   "source": [
    "## Code / Notebook\n",
    "This section contains an overview of the GitHub implementation of Lenna: [GitHub Repo](https://github.com/ujwalkirsan/Lenna_MMDP_Project)\n",
    "\n",
    "```python\n",
    "# Example usage placeholder (this section would include actual demo code if integrated locally)\n",
    "# from lenna import LennaModel\n",
    "# model = LennaModel()\n",
    "# result = model.detect_reason(image_path=\"example.jpg\", question=\"What shows he is playing sports?\")\n",
    "# print(result)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6dbc01",
   "metadata": {},
   "source": [
    "## Reflections\n",
    "**What surprised me?**\n",
    "- The simplicity and effectiveness of using a special token (`<DET>`) for activating object detection mode.\n",
    "- How reasoning-based prompts significantly affect model performance compared to direct object queries.\n",
    "\n",
    "**Scope for improvement:**\n",
    "- Expand reasoning capabilities to video and real-time applications.\n",
    "- Improve generalization to unseen types of implicit reasoning in natural language.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3a650c",
   "metadata": {},
   "source": [
    "## References\n",
    "- Lenna Paper: *Lenna: Language Enhanced Reasoning Detection Assistant*\n",
    "- [GitHub Repository](https://github.com/ujwalkirsan/Lenna_MMDP_Project)\n",
    "- DetGPT, MiniGPT-v2, Grounding-DINO\n",
    "- [BLIP-2 Paper](https://arxiv.org/abs/2301.12597)\n",
    "- [LLaVA: Visual Instruction Tuning](https://arxiv.org/abs/2304.08485)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
